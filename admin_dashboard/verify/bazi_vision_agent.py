# -*- coding: utf-8 -*-
"""
Bazi Vision Agent - ä¸‰å±‚æ™ºèƒ½å…«å­—è¯†åˆ«ç³»ç»Ÿ
Layer 1: Vision Agent - ä½¿ç”¨ MiniMax Vision Pro / GPT-4 Vision / æœ¬åœ°æ¨¡æ‹Ÿ
Layer 2: Normalizer Agent - æ ‡å‡†åŒ–å››æŸ±æ•°æ®
Layer 3: Formatter Agent - æ ¼å¼åŒ–è¾“å‡º
"""

import os
import json
import requests
import base64
from typing import Dict, List, Optional, Tuple
from openai import OpenAI

class BaziVisionAgent:
    """ä¸‰å±‚å…«å­—è¯†åˆ«ä»£ç†ç³»ç»Ÿ"""
    
    def __init__(self):
        self.minimax_api_key = os.getenv("MINIMAX_API_KEY")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_client = OpenAI(api_key=self.openai_api_key) if self.openai_api_key else None
        
        # å¤©å¹²åœ°æ”¯æ˜ å°„
        self.tiangan = ["ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸"]
        self.dizhi = ["å­", "ä¸‘", "å¯…", "å¯", "è¾°", "å·³", "åˆ", "æœª", "ç”³", "é…‰", "æˆŒ", "äº¥"]
        self.wuxing_map = {
            "ç”²": "æœ¨", "ä¹™": "æœ¨", "ä¸™": "ç«", "ä¸": "ç«", "æˆŠ": "åœŸ", 
            "å·±": "åœŸ", "åºš": "é‡‘", "è¾›": "é‡‘", "å£¬": "æ°´", "ç™¸": "æ°´",
            "å­": "æ°´", "ä¸‘": "åœŸ", "å¯…": "æœ¨", "å¯": "æœ¨", "è¾°": "åœŸ", 
            "å·³": "ç«", "åˆ": "ç«", "æœª": "åœŸ", "ç”³": "é‡‘", "é…‰": "é‡‘", 
            "æˆŒ": "åœŸ", "äº¥": "æ°´"
        }
    
    def process_image(self, image_base64: str, progress_callback=None) -> Dict:
        """
        å¤„ç†å…«å­—å›¾ç‰‡ï¼Œè¿”å›è¯†åˆ«ç»“æœ
        
        Args:
            image_base64: Base64 ç¼–ç çš„å›¾ç‰‡æ•°æ®
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(message: str)
        
        Returns:
            è¯†åˆ«ç»“æœå­—å…¸
        """
        try:
            if progress_callback:
                progress_callback("ğŸ¯ å¼€å§‹ä¸‰å±‚æ™ºèƒ½è¯†åˆ«æµç¨‹...")
            
            # Layer 1: Vision Agent - å›¾ç‰‡è¯†åˆ«
            raw_text = self._vision_layer(image_base64, progress_callback)
            
            # Layer 2: Normalizer Agent - æ ‡å‡†åŒ–æ•°æ®
            normalized_data = self._normalizer_layer(raw_text, progress_callback)
            
            # Layer 3: Formatter Agent - æ ¼å¼åŒ–è¾“å‡º
            formatted_result = self._formatter_layer(normalized_data, progress_callback)
            
            if progress_callback:
                progress_callback("âœ… ä¸‰å±‚è¯†åˆ«å®Œæˆï¼")
            
            return {
                "success": True,
                "data": formatted_result,
                "raw_text": raw_text
            }
            
        except Exception as e:
            error_msg = f"è¯†åˆ«å¤±è´¥: {str(e)}"
            if progress_callback:
                progress_callback(f"âŒ {error_msg}")
            return {
                "success": False,
                "error": error_msg
            }
    
    def _vision_layer(self, image_base64: str, progress_callback=None) -> str:
        """Layer 1: Vision Agent - å›¾ç‰‡è¯†åˆ«ï¼ˆä¸‰çº§ fallbackï¼‰"""
        
        # å°è¯• 1: MiniMax Vision Pro
        if self.minimax_api_key:
            try:
                if progress_callback:
                    progress_callback("ğŸ“¸ ä½¿ç”¨ MiniMax Vision Pro è¯†åˆ«...")
                result = self._call_minimax_vision(image_base64)
                if result:
                    if progress_callback:
                        progress_callback("âœ… MiniMax è¯†åˆ«æˆåŠŸ")
                    return result
            except Exception as e:
                if progress_callback:
                    progress_callback(f"âš ï¸ MiniMax å¤±è´¥: {str(e)}")
        
        # å°è¯• 2: GPT-4 Vision
        if self.openai_client:
            try:
                if progress_callback:
                    progress_callback("ğŸ“¸ åˆ‡æ¢åˆ° GPT-4 Vision...")
                result = self._call_gpt4_vision(image_base64)
                if result:
                    if progress_callback:
                        progress_callback("âœ… GPT-4 Vision è¯†åˆ«æˆåŠŸ")
                    return result
            except Exception as e:
                if progress_callback:
                    progress_callback(f"âš ï¸ GPT-4 Vision å¤±è´¥: {str(e)}")
        
        # å°è¯• 3: æœ¬åœ°æ¨¡æ‹Ÿæ•°æ®
        if progress_callback:
            progress_callback("âš™ï¸ ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿæ•°æ®...")
        return self._get_simulated_data()
    
    def _call_minimax_vision(self, image_base64: str) -> Optional[str]:
        """è°ƒç”¨ MiniMax Vision Pro API"""
        url = "https://api.minimax.chat/v1/text/chatcompletion_v2"
        
        # ç§»é™¤å¯èƒ½çš„ data:image å‰ç¼€
        if "," in image_base64:
            image_base64 = image_base64.split(",", 1)[1]
        
        headers = {
            "Authorization": f"Bearer {self.minimax_api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "MiniMax-VL-01",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            }
                        },
                        {
                            "type": "text",
                            "text": """è¯·è¯†åˆ«è¿™å¼ å…«å­—å‘½ç›˜å›¾ç‰‡ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š

1. å››æŸ±å…«å­—ï¼ˆå¹´æŸ±ã€æœˆæŸ±ã€æ—¥æŸ±ã€æ—¶æŸ±ï¼Œæ¯æŸ±åŒ…å«å¤©å¹²å’Œåœ°æ”¯ï¼‰
2. æ€§åˆ«ï¼ˆç”·/å¥³ï¼‰
3. å‡ºç”Ÿæ—¥æœŸå’Œæ—¶é—´ï¼ˆå¦‚æœå›¾ç‰‡ä¸­æœ‰æ˜¾ç¤ºï¼‰

è¯·ä»¥ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
å¹´æŸ±: [å¤©å¹²][åœ°æ”¯]
æœˆæŸ±: [å¤©å¹²][åœ°æ”¯]
æ—¥æŸ±: [å¤©å¹²][åœ°æ”¯]
æ—¶æŸ±: [å¤©å¹²][åœ°æ”¯]
æ€§åˆ«: ç”·/å¥³
å‡ºç”Ÿæ—¶é—´: YYYY-MM-DD HH:MMï¼ˆå¦‚æœæœ‰ï¼‰

åªè¾“å‡ºè¯†åˆ«åˆ°çš„å†…å®¹ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚"""
                        }
                    ]
                }
            ],
            "temperature": 0.1,
            "max_tokens": 500
        }
        
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            if "choices" in data and len(data["choices"]) > 0:
                return data["choices"][0]["message"]["content"]
        
        raise Exception(f"MiniMax API è¿”å›é”™è¯¯: {response.status_code}")
    
    def _call_gpt4_vision(self, image_base64: str) -> Optional[str]:
        """è°ƒç”¨ GPT-4 Vision API"""
        
        if not self.openai_client:
            raise Exception("OpenAI client not initialized")
        
        # ç§»é™¤å¯èƒ½çš„ data:image å‰ç¼€
        if "," in image_base64:
            image_base64 = image_base64.split(",", 1)[1]
        
        response = self.openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            }
                        },
                        {
                            "type": "text",
                            "text": """è¯·è¯†åˆ«è¿™å¼ å…«å­—å‘½ç›˜å›¾ç‰‡ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š

1. å››æŸ±å…«å­—ï¼ˆå¹´æŸ±ã€æœˆæŸ±ã€æ—¥æŸ±ã€æ—¶æŸ±ï¼Œæ¯æŸ±åŒ…å«å¤©å¹²å’Œåœ°æ”¯ï¼‰
2. æ€§åˆ«ï¼ˆç”·/å¥³ï¼‰
3. å‡ºç”Ÿæ—¥æœŸå’Œæ—¶é—´ï¼ˆå¦‚æœå›¾ç‰‡ä¸­æœ‰æ˜¾ç¤ºï¼‰

è¯·ä»¥ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
å¹´æŸ±: [å¤©å¹²][åœ°æ”¯]
æœˆæŸ±: [å¤©å¹²][åœ°æ”¯]
æ—¥æŸ±: [å¤©å¹²][åœ°æ”¯]
æ—¶æŸ±: [å¤©å¹²][åœ°æ”¯]
æ€§åˆ«: ç”·/å¥³
å‡ºç”Ÿæ—¶é—´: YYYY-MM-DD HH:MMï¼ˆå¦‚æœæœ‰ï¼‰

åªè¾“å‡ºè¯†åˆ«åˆ°çš„å†…å®¹ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚"""
                        }
                    ]
                }
            ],
            temperature=0.1,
            max_tokens=500
        )
        
        return response.choices[0].message.content
    
    def _get_simulated_data(self) -> str:
        """è¿”å›æ¨¡æ‹Ÿå…«å­—æ•°æ®"""
        return """å¹´æŸ±: åºšè¾°
æœˆæŸ±: å·±å¯
æ—¥æŸ±: ç”²å­
æ—¶æŸ±: ä¹™ä¸‘
æ€§åˆ«: ç”·
å‡ºç”Ÿæ—¶é—´: 2000-03-15 10:30"""
    
    def _normalizer_layer(self, raw_text: str, progress_callback=None) -> Dict:
        """Layer 2: Normalizer Agent - æ ‡å‡†åŒ–æ•°æ®"""
        
        if progress_callback:
            progress_callback("ğŸ”§ æ ‡å‡†åŒ–å››æŸ±æ•°æ®...")
        
        result = {
            "year_gan": "", "year_zhi": "",
            "month_gan": "", "month_zhi": "",
            "day_gan": "", "day_zhi": "",
            "hour_gan": "", "hour_zhi": "",
            "gender": "",
            "birth_time": ""
        }
        
        lines = raw_text.strip().split("\n")
        
        for line in lines:
            line = line.strip()
            
            if "å¹´æŸ±" in line or "Year" in line:
                chars = self._extract_ganzhi(line)
                if len(chars) >= 2:
                    result["year_gan"], result["year_zhi"] = chars[0], chars[1]
            
            elif "æœˆæŸ±" in line or "Month" in line:
                chars = self._extract_ganzhi(line)
                if len(chars) >= 2:
                    result["month_gan"], result["month_zhi"] = chars[0], chars[1]
            
            elif "æ—¥æŸ±" in line or "Day" in line:
                chars = self._extract_ganzhi(line)
                if len(chars) >= 2:
                    result["day_gan"], result["day_zhi"] = chars[0], chars[1]
            
            elif "æ—¶æŸ±" in line or "Hour" in line:
                chars = self._extract_ganzhi(line)
                if len(chars) >= 2:
                    result["hour_gan"], result["hour_zhi"] = chars[0], chars[1]
            
            elif "æ€§åˆ«" in line or "Gender" in line:
                if "ç”·" in line or "Male" in line:
                    result["gender"] = "ç”·"
                elif "å¥³" in line or "Female" in line:
                    result["gender"] = "å¥³"
            
            elif "å‡ºç”Ÿæ—¶é—´" in line or "Birth" in line:
                parts = line.split(":", 1)
                if len(parts) > 1:
                    result["birth_time"] = parts[1].strip()
        
        if progress_callback:
            progress_callback(f"âœ… è¯†åˆ«åˆ°: {result['year_gan']}{result['year_zhi']} {result['month_gan']}{result['month_zhi']} {result['day_gan']}{result['day_zhi']} {result['hour_gan']}{result['hour_zhi']}")
        
        return result
    
    def _extract_ganzhi(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å¹²æ”¯å­—ç¬¦"""
        chars = []
        all_chars = self.tiangan + self.dizhi
        
        for char in text:
            if char in all_chars:
                chars.append(char)
        
        return chars
    
    def _formatter_layer(self, normalized_data: Dict, progress_callback=None) -> Dict:
        """Layer 3: Formatter Agent - æ ¼å¼åŒ–è¾“å‡º"""
        
        if progress_callback:
            progress_callback("ğŸ“¦ æ ¼å¼åŒ–è¾“å‡ºæ•°æ®...")
        
        # è®¡ç®—äº”è¡Œ
        wuxing = self._calculate_wuxing(normalized_data)
        
        result = {
            "bazi": {
                "year": f"{normalized_data['year_gan']}{normalized_data['year_zhi']}",
                "month": f"{normalized_data['month_gan']}{normalized_data['month_zhi']}",
                "day": f"{normalized_data['day_gan']}{normalized_data['day_zhi']}",
                "hour": f"{normalized_data['hour_gan']}{normalized_data['hour_zhi']}"
            },
            "pillars": {
                "year_gan": normalized_data["year_gan"],
                "year_zhi": normalized_data["year_zhi"],
                "month_gan": normalized_data["month_gan"],
                "month_zhi": normalized_data["month_zhi"],
                "day_gan": normalized_data["day_gan"],
                "day_zhi": normalized_data["day_zhi"],
                "hour_gan": normalized_data["hour_gan"],
                "hour_zhi": normalized_data["hour_zhi"]
            },
            "gender": normalized_data.get("gender", ""),
            "birth_time": normalized_data.get("birth_time", ""),
            "wuxing": wuxing
        }
        
        return result
    
    def _calculate_wuxing(self, data: Dict) -> Dict:
        """è®¡ç®—äº”è¡Œåˆ†å¸ƒ"""
        wuxing_count = {"é‡‘": 0, "æœ¨": 0, "æ°´": 0, "ç«": 0, "åœŸ": 0}
        
        chars = [
            data["year_gan"], data["year_zhi"],
            data["month_gan"], data["month_zhi"],
            data["day_gan"], data["day_zhi"],
            data["hour_gan"], data["hour_zhi"]
        ]
        
        for char in chars:
            if char in self.wuxing_map:
                wuxing = self.wuxing_map[char]
                wuxing_count[wuxing] += 1
        
        return wuxing_count


# ä¾¿æ·å‡½æ•°
def process_bazi_image(image_base64: str, progress_callback=None) -> Dict:
    """
    å¤„ç†å…«å­—å›¾ç‰‡çš„ä¾¿æ·å‡½æ•°
    
    Args:
        image_base64: Base64 ç¼–ç çš„å›¾ç‰‡æ•°æ®
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    
    Returns:
        è¯†åˆ«ç»“æœå­—å…¸
    """
    agent = BaziVisionAgent()
    return agent.process_image(image_base64, progress_callback)
